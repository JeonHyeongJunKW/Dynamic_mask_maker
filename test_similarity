import cv2
import glob
import numpy as np
from custom_vision_tool import *
import pickle
import os
from custom_metric import *
from sklearn.cluster import DBSCAN
Lambda_1 = 0.15
Lambda_2 = 0.4
Lambda_3 = 0#0.45
sigma_c = 4.8
sigma_g = 0.25
sigma_h = 4.8
# Lambda_4 = Lambda_1/(Lambda_1+Lambda_2)
# Lambda_5 = Lambda_2/(Lambda_1+Lambda_2)
Lambda_6 = 0.12
Lambda_7 = 0.36
Lambda_8 = 0.03
t_r =0.6

patchsize = 32 # 특징추출 및 이미지 갱신에 사용되는 패치사이즈입니다.

butterfly_image = glob.glob("D:/Davis dataset/DAVIS/JPEGImages/480p/butterfly/*.jpg")
sampled_butterfly_image = [butterfly_image[i]  for i in range(0,len(butterfly_image), 10)]#10개정도의 샘플이미지에서 장소이미지를 얻어옵니다.
saved_color_imgs = []
saved_imgs_gray = []
correspondence_map = []#denseflow에 대한 매핑입니다.
lab_images = [] # lab mean feature에 대한 매핑입니다.(소스이미지에 대한 매핑만이 존재합니다.)
#load ref image and source image
Extracted_feature = [0 for _ in range(len(sampled_butterfly_image))]#feature 추출은 모든 이미지(ref, source)에 대해서 합니다.
fundamental_mats = []#각각의 fundamental matrix입니다.
for idx, image_name in enumerate(sampled_butterfly_image):
    color_bgr_img = cv2.imread(image_name)
    color_origin_img = color_bgr_img.copy()
    saved_color_imgs.append(color_origin_img)

    gray_img = cv2.cvtColor(color_bgr_img, cv2.COLOR_BGR2GRAY)
    saved_imgs_gray.append(gray_img)
    Extracted_feature[idx] = {}
    if not os.path.isfile('denseSIFT.pickle'):
        print(idx, "번째 이미지를 읽는중입니다.")
        for y in range(color_bgr_img.shape[0]):
            for x in range(color_bgr_img.shape[1]):
                des = extract_SIFT(gray_img, x, y, patchsize)#패치사이즈 만하게 descriptor를 뽑습니다.
                if len(des) == 0:#가장자리 영역이면 해당 descriptor를 0으로 초기화합니다.
                    Extracted_feature[idx][(y, x)] = 0
                else:#아니라면 해당 descriptor를 descriptor로 초기화합니다. 1 x120이엇나..
                    Extracted_feature[idx][(y, x)] =des

if not os.path.isfile('denseSIFT.pickle'):
    with open('denseSIFT.pickle', 'wb') as f:
        pickle.dump(Extracted_feature,f,pickle.HIGHEST_PROTOCOL)
else:
    with open('denseSIFT.pickle', 'rb') as f:
        Extracted_feature = pickle.load(f)

ref_img = saved_imgs_gray[0]#기존 gray성격의 reference 이미지 입니다.
return_img = saved_color_imgs[0].copy()#수정되는 color형태의 이미지입니다.
img_h, img_w = ref_img.shape
mask_img = np.ones((img_h,img_w))*255#이미지 마스킹간에 사용되는 마스크입니다.
source_imgs = saved_imgs_gray[1:]#첫번째 원소를 제외한 소스이미지입니다.


similarity_map =np.zeros((img_h,img_w,len(source_imgs)))#similarity map의 크기는 source 이미지 개수
for source_img in source_imgs:
    fundamental_matrix = Get_Fundamental(ref_img,source_img)
    fundamental_mats.append(fundamental_matrix)


for source_img in source_imgs:
    #get denseflow
    dense_flow = cv2.calcOpticalFlowFarneback(ref_img, source_img,None,0.5,3,13,10,5,1.1,cv2.OPTFLOW_FARNEBACK_GAUSSIAN)
    correspondence_map.append(dense_flow)

for image in saved_color_imgs:
    #get lad feature
    lab_image = cv2.cvtColor(image,cv2.COLOR_BGR2LAB)
    kernel = np.ones((patchsize,patchsize),np.float32)/(patchsize*patchsize)
    lab_image = cv2.filter2D(lab_image,-1,kernel)
    lab_images.append(lab_image)



if not os.path.isfile('similarity_map1.pickle'):
    for idx, source_img in enumerate(source_imgs):#일종에 신뢰도를 구하기 위한 전처리과정인데 너무 오래걸린다.
        print("making similarity map :",idx)
        for y in range(source_img.shape[0]):
            for x in range(source_img.shape[1]):
                #Extracted_feature의 값이 numpy인지를 확인합니다.
                # 아니라면 similarity_map을 채우지않고 넘어갑니다.
                if type(Extracted_feature[0][(y, x)]) is not np.ndarray:
                    continue

                x_hat = np.array([y, x]) + np.flip(correspondence_map[idx][y, x])
                x_hat_int = x_hat.astype(np.int32)
                if(x_hat_int[0]<0 or x_hat_int[1]<0) or (x_hat_int[0] >= img_h or x_hat_int[1]>= img_w):
                    continue#이미지 밖으로 투영된 점
                if type(Extracted_feature[idx+1][(x_hat_int[0], x_hat_int[1])]) is not np.ndarray:
                    similarity_map[y, x, idx] =0
                    continue
                f_c_x_r = lab_images[0][y, x, :]#ref이미지에 대한 LAB벡터입니다.
                f_c_x_hat = lab_images[idx+1][x_hat_int[0],x_hat_int[1],:]#source 이미지에 대한 LAB벡터입니다.

                f_g_x_r = Extracted_feature[0][(y, x)]#ref이미지에 대한 dense sift입니다.
                f_g_x_hat = Extracted_feature[idx+1][(x_hat_int[0],x_hat_int[1])]  # ref이미지에 대한 dense sift입니다.

                dist_1 =Se_distance(f_c_x_r,f_c_x_hat,sigma_c)
                dist_2 =Se_distance(f_g_x_r, f_g_x_hat, sigma_g)
                dist_3 = Sf_distance(np.float64([x, y, 1]), np.float64([x_hat[1], x_hat[0],1]), fundamental_mats[idx])
                similarity_map[y, x, idx] = Lambda_1 * dist_1 + \
                                            Lambda_2 * dist_2 + \
                                            Lambda_3 * dist_3
        max_sim = np.max(similarity_map[:, :,idx])
        print("최대값 ",max_sim)
        print("최대색깔", similarity_map[100:200, 100:200, idx])
        image_sim = 255.*similarity_map[:, :, idx]
        # print()
        print(image_sim[100:200, 100:200])
        print(image_sim[100:200, 100:200]/max_sim)
        normalized_image = image_sim.astype(np.uint8)
        print("정수로 바꾼이미지 ")
        print(normalized_image)
        print("updated new image")
        cv2.imshow("origin_image",saved_color_imgs[idx+1])
        cv2.imshow("similaritymap",normalized_image)
        cv2.waitKey(0)